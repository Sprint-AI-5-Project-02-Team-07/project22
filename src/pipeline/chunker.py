import json
import re
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import List, Dict, Optional, Tuple

# =========================
# ì„¤ì •(í•„ìš”ì‹œ ì¡°ì •)
# =========================
MAX_CHARS_PER_CHUNK = 4500     # ì„ë² ë”©/LLM ì»¨í…ìŠ¤íŠ¸ì— ë§ê²Œ ì¡°ì ˆ
MIN_CHARS_PER_CHUNK = 400      # ë„ˆë¬´ ì§§ì€ ì¡°ê° í•©ì¹˜ê¸° ê¸°ì¤€
SOFT_SPLIT_MAX_CHARS = 2200    # ì•„ì£¼ ê¸´ chunkë¥¼ ì¶”ê°€ë¡œ ë¶€ë“œëŸ½ê²Œ ìª¼ê°¤ ë•Œ ê¸°ì¤€
DOT_RATIO_TH = 0.35            # ì ì„  ë¹„ìœ¨ ì„ê³„ì¹˜ (Cleaning)
MIN_TEXT_LEN_CLEAN = 50        # Cleaning ë‹¨ê³„ ìµœì†Œ ê¸¸ì´

# =========================
# Cleaning Logic (from old text_cleaner.py)
# =========================
def remove_decorative_lines(text: str) -> str:
    """ì ì„ /ì¥ì‹ ìœ„ì£¼ ë¼ì¸ ì œê±°"""
    lines = []
    for line in text.splitlines():
        l = line.strip()
        if not l: continue
        if re.fullmatch(r"[Â·\.\-\s]+", l): continue
        lines.append(line)
    return "\n".join(lines).strip()

def is_toc_chunk(text: str) -> bool:
    """ëª©ì°¨(TOC) íœ´ë¦¬ìŠ¤í‹± íŒë³„"""
    lines = [l.strip() for l in text.splitlines() if l.strip()]
    if not lines: return True
    if any("ëª© ì°¨" in l or "ëª©ì°¨" in l for l in lines[:3]): return True

    dot_lines = sum(1 for l in lines if "Â·" in l)
    digit_lines = sum(1 for l in lines if any(c.isdigit() for c in l))
    long_text_lines = sum(1 for l in lines if len(l) >= 25 and "Â·" not in l)

    if dot_lines / len(lines) > DOT_RATIO_TH and long_text_lines < 3: return True
    if digit_lines / len(lines) > 0.7 and long_text_lines < 3: return True
    return False

def clean_text_block(text: str) -> str | None:
    """
    í…ìŠ¤íŠ¸ ë¸”ë¡ ì •ì œ. 
    ì˜ë¯¸ê°€ ì—†ìœ¼ë©´ None ë°˜í™˜.
    """
    # 1) ì¥ì‹ ì œê±°
    cleaned = remove_decorative_lines(text)
    
    # 2) TOC ì²´í¬
    if is_toc_chunk(cleaned):
        return None
        
    # 3) ê¸¸ì´ ì²´í¬
    if len(cleaned) < MIN_TEXT_LEN_CLEAN:
        return None
        
    return cleaned


# =========================
# Splitting Logic (from old json2jsonl.py)
# =========================

# 1) ì„¹ì…˜/í—¤ë” í›„ë³´
SECTION_TITLE_PATTERNS = [
    r"ì‚¬ì—…\s*ê°œìš”", r"ì‚¬ì—…\s*ëª©ì ", r"ì¶”ì§„\s*ë°°ê²½",
    r"ì‚¬ì—…\s*ë²”ìœ„", r"ê³¼ì—…\s*ë²”ìœ„", r"ê³¼ì—…\s*ë‚´ìš©", r"ì—…ë¬´\s*ë²”ìœ„",
    r"ì œì•ˆ\s*ìš”ì²­\s*ì‚¬í•­", r"ì œì•ˆ\s*ì„œ\s*ì‘ì„±", r"ì œì•ˆì„œ\s*ì‘ì„±",
    r"í‰ê°€\s*ê¸°ì¤€", r"í‰ê°€\s*ë°©ë²•", r"ì„ ì •\s*ê¸°ì¤€", r"ì‹¬ì‚¬\s*ê¸°ì¤€",
    r"ì œì¶œ\s*ì„œë¥˜", r"ì…ì°°\s*ì°¸ê°€", r"ì…ì°°\s*ë°©ë²•", r"ê³„ì•½\s*ì¡°ê±´",
    r"ì¼ì •", r"ì¶”ì§„\s*ì¼ì •", r"ìˆ˜í–‰\s*ì¼ì •",
    r"ìœ ì˜\s*ì‚¬í•­", r"ê¸°íƒ€\s*ì‚¬í•­", r"ì°¸ê³ \s*ì‚¬í•­",
    r"ì§ˆì˜\s*ì‘ë‹µ", r"ë¬¸ì˜ì²˜",
]
SECTION_TITLE_RE = re.compile(
    r"^(?:#+\s*)?(?P<title>(" + "|".join(SECTION_TITLE_PATTERNS) + r"))\s*$"
)
CLAUSE_RE = re.compile(r"^(?P<key>ì œ\s*\d+\s*ì¡°)\b.*$")
NUMBERED_RE = re.compile(
    r"^(?P<key>(?:\(?\d+\)?[.)]|(?:\d+\s*-\s*\d+)|[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]))\s+.*$"
)
APPENDIX_RE = re.compile(r"^(?P<key>(?:ë¶€ë¡|ë³„ì²¨|ì²¨ë¶€|ë¶™ì„))\b.*$")

@dataclass
class Page:
    page: int
    content: str
    metadata: Dict

@dataclass
class Chunk:
    source_pdf: str
    chunk_id: str
    section_title: Optional[str]
    clause_key: Optional[str]
    page_start: int
    page_end: int
    text: str # content

def load_pages(parsed_json_path: Path) -> List[Page]:
    with parsed_json_path.open("r", encoding="utf-8") as f:
        data = json.load(f)
    
    pages: List[Page] = []
    for item in data:
        # page: global page (top-level) ë¥¼ ìš°ì„  ì‚¬ìš©
        p = int(item.get("page") or item.get("metadata", {}).get("global_page") or 0)
        content = item.get("content") or ""
        md = item.get("metadata") or {}
        pages.append(Page(page=p, content=content, metadata=md))
    pages.sort(key=lambda x: x.page)
    return pages

def normalize_line(line: str) -> str:
    return line.rstrip()

def is_markdown_table_line(line: str) -> bool:
    return "|" in line

def detect_boundary(line: str) -> Tuple[Optional[str], Optional[str]]:
    l = line.strip()
    if not l: return None, None

    m = SECTION_TITLE_RE.match(l)
    if m: return m.group("title"), None

    m = CLAUSE_RE.match(l)
    if m: return None, m.group("key").replace(" ", "")

    m = APPENDIX_RE.match(l)
    if m: return None, m.group("key")

    m = NUMBERED_RE.match(l)
    if m: return None, m.group("key").replace(" ", "")

    return None, None

def soft_split_text(text: str, max_chars: int = SOFT_SPLIT_MAX_CHARS) -> List[str]:
    if len(text) <= max_chars:
        return [text]

    blocks = text.split("\n\n")
    out, buf = [], ""

    def flush():
        nonlocal buf
        if buf.strip():
            out.append(buf.strip())
        buf = ""

    for b in blocks:
        b = b.strip()
        if not b: continue

        # í‘œ ë¸”ë¡ ìœ ì§€
        if any("|" in line for line in b.splitlines()):
            if len(buf) + len(b) > max_chars: flush()
            buf += b + "\n\n"
            continue

        # look-behind ì œê±°í•œ ë¬¸ì¥ ë¶„ë¦¬
        parts = re.split(r"([.?!]|ë‹¤\.)\s+", b)
        sentences = []
        for i in range(0, len(parts) - 1, 2):
            sentences.append(parts[i] + parts[i + 1])

        for s in sentences:
            if len(buf) + len(s) <= max_chars:
                buf += s + " "
            else:
                flush()
                buf += s + " "
    flush()
    return out

def split_pages_into_chunks(pages: List[Page], source_pdf: str) -> List[Chunk]:
    chunks: List[Chunk] = []

    current_section: Optional[str] = None
    current_clause: Optional[str] = None

    buf_lines: List[str] = []
    buf_page_start: Optional[int] = None
    buf_page_end: Optional[int] = None

    def start_buffer(page_no: int):
        nonlocal buf_page_start, buf_page_end
        if buf_page_start is None:
            buf_page_start = page_no
        buf_page_end = page_no

    def flush_buffer(force: bool = False):
        nonlocal buf_lines, buf_page_start, buf_page_end, chunks
        
        # ğŸŸ¢ Apply cleaning before making chunk
        raw_text = "\n".join(buf_lines).strip()
        cleaned_text = clean_text_block(raw_text) # can be None if empty/garbage
        
        if not cleaned_text:
            # Reset buffer only
            buf_lines = []
            buf_page_start = None
            buf_page_end = None
            return

        # ë„ˆë¬´ ì§§ìœ¼ë©´ ì´ì „ chunkì— í•©ì¹˜ê¸°(ê°€ëŠ¥í•  ë•Œ)
        if len(cleaned_text) < MIN_CHARS_PER_CHUNK and chunks and not force:
            prev = chunks[-1]
            prev.text = (prev.text.rstrip() + "\n\n" + cleaned_text).strip()
            prev.page_end = max(prev.page_end, buf_page_end or prev.page_end)
        else:
            chunk_id = f"{Path(source_pdf).stem}__p{buf_page_start:04d}-{buf_page_end:04d}__{len(chunks)+1:05d}"
            chunks.append(Chunk(
                source_pdf=source_pdf,
                chunk_id=chunk_id,
                section_title=current_section,
                clause_key=current_clause,
                page_start=buf_page_start or 0,
                page_end=buf_page_end or (buf_page_start or 0),
                text=cleaned_text # Use cleaned Text
            ))

        buf_lines = []
        buf_page_start = None
        buf_page_end = None

    for page in pages:
        page_no = page.page
        content = page.content or ""
        # ğŸŸ¢ Basic Line Normalization
        lines = [normalize_line(x) for x in content.splitlines()]

        if not any(l.strip() for l in lines): continue

        for line in lines:
            sec, clause = detect_boundary(line)

            if sec or clause:
                in_table_context = (
                    len(buf_lines) >= 2
                    and is_markdown_table_line(buf_lines[-1])
                    and is_markdown_table_line(buf_lines[-2])
                )
                if not in_table_context:
                    flush_buffer()

                if sec:
                    current_section = sec
                    current_clause = None
                if clause:
                    current_clause = clause

            start_buffer(page_no)
            buf_lines.append(line)

            if sum(len(x) for x in buf_lines) > MAX_CHARS_PER_CHUNK:
                flush_buffer(force=True)

    flush_buffer(force=True)
    
    # Final Size Check & Soft Split
    final_chunks: List[Chunk] = []
    for c in chunks:
        if len(c.text) <= MAX_CHARS_PER_CHUNK:
            final_chunks.append(c)
        else:
            parts = soft_split_text(c.text, max_chars=SOFT_SPLIT_MAX_CHARS)
            for idx, part in enumerate(parts, start=1):
                cid = f"{c.chunk_id}__s{idx:02d}"
                # content key for loader
                final_chunks.append(Chunk(
                    source_pdf=c.source_pdf,
                    chunk_id=cid,
                    section_title=c.section_title,
                    clause_key=c.clause_key,
                    page_start=c.page_start,
                    page_end=c.page_end,
                    text=part
                ))
    return final_chunks

# =========================
# Main Execution
# =========================
def process_file(parsed_json_path: Path, out_path: Path):
    try:
        pages = load_pages(parsed_json_path)
    except Exception as e:
        print(f"Error loading {parsed_json_path}: {e}")
        return

    # source_pdf ì¶”ë¡ 
    source_pdf = None
    if pages:
        source_pdf = pages[0].metadata.get("source_pdf")
    source_pdf = source_pdf or (parsed_json_path.stem.replace("_parsed", "") + ".pdf")

    chunks = split_pages_into_chunks(pages, source_pdf=source_pdf)
    
    # Save as JSONL
    # Loader expects: content, page, metadata
    with out_path.open("w", encoding="utf-8") as f:
        for c in chunks:
            # Map Chunk -> Loader Compatible Dict
            out_obj = {
                "content": c.text,
                "page": c.page_start, # Representative page
                "metadata": {
                    "source_pdf": c.source_pdf,
                    "chunk_id": c.chunk_id,
                    "section_title": c.section_title,
                    "clause_key": c.clause_key,
                    "page_start": c.page_start,
                    "page_end": c.page_end
                }
            }
            f.write(json.dumps(out_obj, ensure_ascii=False) + "\n")
            
    print(f"âœ… Chunked: {parsed_json_path.name} -> {len(chunks)} chunks")

def run_chunking(input_dir: str, output_dir: str):
    in_dir = Path(input_dir)
    out_dir = Path(output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    files = sorted(in_dir.glob("*_parsed.json"))
    print(f"[Chunker] Found {len(files)} parsed files in {in_dir}")

    for f in files:
        out_name = f.stem.replace("_parsed", "_clean") + ".jsonl"
        out_path = out_dir / out_name
        process_file(f, out_path)
