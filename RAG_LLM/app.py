import streamlit as st
import time
import os
from dotenv import load_dotenv

# Load Env
load_dotenv()

# Internal Modules
from src.loader import load_data
from src.retrieval import initialize_hybrid_retriever, retrieve_documents
from src.generation import generate_answer
from src.query_extractor import extract_filters
from src.session_manager import get_merged_filters, update_context
from src.decomposition import decompose_query

# ---------------------------------------------------------
# Page Config
# ---------------------------------------------------------
st.set_page_config(
    page_title="RAG Chatbot",
    page_icon="ğŸ¤–",
    layout="wide"
)

st.title("ğŸ¤– AI ì‚¬ì—… ê³µê³  ë¶„ì„ ì±—ë´‡")

# ---------------------------------------------------------
# Sidebar: Settings & Manual Filters
# ---------------------------------------------------------
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì • (Settings)")
    
    # Force Reload Button
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ”„ ìºì‹œ ì´ˆê¸°í™”"):
            st.cache_resource.clear()
            st.rerun()
            
    with col2:
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
            st.session_state.messages = []
            # Remove session file
            import src.session_manager as sm
            if os.path.exists(sm.SESSION_FILE):
                os.remove(sm.SESSION_FILE)
            st.rerun()
        
    st.divider()
    
    st.subheader("ğŸ” ìˆ˜ë™ í•„í„° (Optional)")
    manual_agency = st.text_input("ê¸°ê´€ëª… (Agency)", placeholder="ì˜ˆ: í‰íƒì‹œ")
    manual_amount = st.number_input("ìµœì†Œ ê¸ˆì•¡ (Amount)", min_value=0, step=1000000, value=0)

# ---------------------------------------------------------
# Initialization (Cached)
# ---------------------------------------------------------
@st.cache_resource
def get_cached_documents(version):
    # version param is used purely to force cache invalidation when changed in config
    st.write(f"Cache Version: {version} - Reloading Data...")
    return load_data(use_cache=True)

@st.cache_resource
def get_cached_retriever(_docs):
    # This function builds the retriever and returns it.
    # Streamlit will cache the resulting object.
    initialize_hybrid_retriever(_docs)
    from src.retrieval import _hybrid_retriever
    return _hybrid_retriever

def initialize_system():
    with st.spinner("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘... (ë¬¸ì„œ ë¡œë”© & ì¸ë±ì‹±)"):
        # 1. Load Data
        docs = get_cached_documents(config.CACHE_VERSION)
        
        # 2. Init Retriever (Cached)
        # We need to ensure the global variable in src.retrieval is set
        # because retrieve_documents() uses it.
        retriever = get_cached_retriever(docs)
        
        # FORCE set the global variable in src.retrieval
        import src.retrieval
        src.retrieval._hybrid_retriever = retriever
        
        return True

# ---------------------------------------------------------
# Chat History Management
# ---------------------------------------------------------
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display Chat History
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        # If there are source documents (only for assistant), show them
        if message.get("sources"):
             with st.expander("ğŸ“š ì°¸ê³  ë¬¸ì„œ (Sources) í™•ì¸í•˜ê¸°"):
                for idx, doc in enumerate(message["sources"]):
                    st.markdown(f"**{idx+1}. [{doc['metadata'].get('agency', 'Unknown')}] {doc['metadata'].get('title', 'Untitled')}**")
                    st.text(doc['page_content'][:300] + "...")
                    st.divider()

# ---------------------------------------------------------
# Chat Logic
# ---------------------------------------------------------
if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”... (ì˜ˆ: í‰íƒì‹œ ë²„ìŠ¤ ì˜ˆì‚° ì–¼ë§ˆì•¼?)"):
    # 1. User Message
    with st.chat_message("user"):
        st.markdown(query)
    st.session_state.messages.append({"role": "user", "content": query})

    # 2. Assistant Logic
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):

            # A. Auto-Filter Extraction
            # We extract filters for the *original* query to capture context (like agency if mentioned globally)
            # But decomposition handles specific entities better.
            auto_filters = extract_filters(query)
            
            # B. Merge with Session
            merged_filters = get_merged_filters(auto_filters)
            if manual_agency:
                merged_filters['agency'] = manual_agency
            if manual_amount > 0:
                merged_filters['min_amount'] = manual_amount
                
            if merged_filters:
                st.caption(f"ğŸ’¡ **ê¸°ë³¸ í•„í„°**: {merged_filters}")
                update_context(merged_filters, query)

            # C. Query Decomposition & Retrieval
            sub_queries = decompose_query(query)
            
            all_retrieved_docs = []
            seen_contents = set()
            
            # Progress bar if multiple steps
            if len(sub_queries) > 1:
                st.info(f"ğŸ§© ë³µì¡í•œ ì§ˆë¬¸ì´ë„¤ìš”! ë‹¤ìŒ {len(sub_queries)}ê°€ì§€ë¡œ ë‚˜ëˆ„ì–´ ê²€ìƒ‰í•©ë‹ˆë‹¤: {sub_queries}")
                my_bar = st.progress(0)
            
            for i, sub_q in enumerate(sub_queries):
                # For sub-queries, we might NOT want to enforce the global sticky agency filter 
                # if the sub-query explicitly mentions a different agency.
                # However, our retrieval logic's deep fallback uses the filter.
                # For simplicity, let's use the merged filter BUT be aware.
                # Actually, if sub-query is "Ulsan budget", and sticky filter is "Pyeongtaek", 
                # we have a conflict.
                # Ideally, decomposition should override filters. 
                # Let's pass 'None' for filters if sub-queries are used, 
                # OR let the retrieval logic handle it.
                # Current Decision: Pass merged_filters. If conflict, retrieval might fail, but fallback searches vector.
                
                # BETTER: Modify extractor to run on sub-query? 
                # Let's just run retrieval with merged_filters for now.
                
                docs = retrieve_documents(sub_q, filter_criteria=merged_filters if len(sub_queries) == 1 else None)
                
                for doc in docs:
                    if doc.page_content not in seen_contents:
                        seen_contents.add(doc.page_content)
                        all_retrieved_docs.append(doc)
                
                if len(sub_queries) > 1:
                    my_bar.progress((i + 1) / len(sub_queries))
            
            if len(sub_queries) > 1:
                my_bar.empty()
            
            # --- DEBUG: Show All Retrieved Candidates ---
            with st.expander("ğŸ•µï¸ ë””ë²„ê¹…: ê²€ìƒ‰ëœ ëª¨ë“  ë¬¸ì„œ (Reranking ì „í›„)", expanded=False):
                st.write(f"Total Candidates: {len(all_retrieved_docs)}")
                for i, doc in enumerate(all_retrieved_docs):
                    st.text(f"[{i+1}] {doc.page_content[:100]}...")
            # --------------------------------------------
            
            # D. Generate Answer (Synthesis)
            # We pass the ORIGINAL query, but with ALL retrieved documents.
            if not all_retrieved_docs:
                st.warning("âš ï¸ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                answer = "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            else:
                answer = generate_answer(query, all_retrieved_docs)
            
            message_placeholder.markdown(answer)
            
            # Prepare source metadata for history
            sources_clean = []
            for doc in all_retrieved_docs:
                sources_clean.append({
                    "page_content": doc.page_content,
                    "metadata": doc.metadata
                })
            
            # E. Show Sources in Expander (Current Turn)
            with st.expander("ğŸ“š ì°¸ê³  ë¬¸ì„œ (Sources) - Click to expand"):
                for idx, doc in enumerate(all_retrieved_docs):
                    st.markdown(f"**{idx+1}. [{doc.metadata.get('agency', 'Unknown')}] {doc.metadata.get('title', 'Unknown')}**")
                    st.caption(f"Score/Rank: {idx+1}")
                    st.text(doc.page_content[:400] + "...")
                    st.divider()

    # 3. Save Assistant Message
    st.session_state.messages.append({
        "role": "assistant", 
        "content": answer,
        "sources": sources_clean
    })
